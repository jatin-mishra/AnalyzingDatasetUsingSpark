agent.sources = source
agent.sinks = sinkOne sinkTwo
agent.channels = channelOne channelTwo


# source
agent.sources.source.type = exec
agent.sources.source.command = tail -F /opt/gen_logs/access.log


# sinkOne
agent.sinks.sinkOne.type=hdfs
agent.sinks.sinkOne.hdfs.path = hdfs://domainName:8020/user/username/dirname
agent.sinks.sinkOne.hdfs.filePrefix = FlumeDemo
agent.sinks.sinkOne.hdfs.fileSuffix = .txt
agent.sinks.sinkOne.hdfs.rollInterval = 120
agent.sinks.sinkOne.hdfs.rollSize = 1048576
agent.sinks.sinkOne.hdfs.rollCount = 100
agent.sinks.sinkOne.hdfs.fileType = DataStream

# sinkTwo
agent.sinks.sinkTwo.type = org.apache.spark.streaming.flume.sink.SparkSink
agent.sinks.sinkTwo.hostname = <hostnameOfLocalMachine>
agent.sinks.sinkTwo.port = <port>

# channelOne
agent.channels.channelOne.type = memory
agent.channels.channelOne.capacity = 1000
agent.channels.channelOne.transactionCapacity = 100

# channelTwo
agent.channels.channelTwo.type = memory
agent.channels.channelTwo.capacity = 1000
agent.channels.channelTwo.transactionCapacity = 100

# source to channels
agent.sources.source.channels = channelOne channelTwo

# sinks to channels
agent.sinks.sinkTwo.channel = channelTwo
agent.sinks.sinkOne.channel = channelOne

# now add dependies at build.sbt and integrate spark and flume

